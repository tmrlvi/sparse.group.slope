---
title: "Cancer Sites"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{simulation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(GEOquery)
library(dplyr)
library(stringr)
options(error = quote(dump.frames()))
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Process Data

```{r}
gse <- getGEO("GSE2564",GSEMatrix=TRUE)
```

```{r}
usedFeatures <- featureData(gse[[1]])$USAGE == "Used"
usedSampled <- phenoData(gse[[1]])$organism_ch1 == "Homo sapiens"
limited <- gse[[1]][usedFeatures,usedSampled]

classes <- pData(phenoData(limited)) %>% 
  mutate(class=str_extract(title, "[^_]+_[^_]+")) %>% 
  group_by(class) %>% 
  mutate(count=n()) %>% 
  ungroup() %>% 
  mutate(tumor_or_normal=str_starts(class, "T_") | str_starts(class, "N_")) %>%
  select(geo_accession, class, count, tumor_or_normal)
  
selected <- gse[[1]][, classes$geo_accession[(classes$count >= 5) & classes$tumor_or_normal]]

X <- t(exprs(selected))
X <- X[,!apply((X == 0), 2, all)]
y <- as.factor((pData(phenoData(selected)) %>% 
  mutate(class=str_extract(title, "[^_]+_[^_]+")) %>% 
  select(class))$class)
```

# Sparse Group Lasso
```{r}
msgl.fit.cv <- function(X, y, grid.alpha, grid.lambda, cv.indices) {
    library(msgl)
    Xorig <- X
    X <- X[,!apply(X, 2, function(x){all(x == x[1])})]
    cvs <- matrix(-1, length(grid.alpha), length(grid.lambda))

    cl <- parallel::makeCluster(5)
    doParallel::registerDoParallel(cl)
    for (i in 1:length(grid.alpha)) {
        model <- cv(
            X, y, fold = length(cv.indices), lambda = grid.lambda, 
            alpha = grid.alpha[i], use_parallel = TRUE, standardize = TRUE, 
            cv.indices = cv.indices
        )
        cvs[i,] <- Err(model)
    }
    parallel::stopCluster(cl)
    return(cvs)
}

msgl.fit_evaluate <- function(Xtrain, ytrain, Xtest, ytest, alpha, lambda) {
  library(msgl)
  Xorig <- Xtrain
  Xtest <- Xtest[,!apply(Xtrain, 2, function(x){all(x == x[1])})]
  Xtrain <- Xtrain[,!apply(Xtrain, 2, function(x){all(x == x[1])})]
  model <- msgl::fit(
    Xtrain, ytrain, alpha = alpha, lambda = c(lambda, lambda), 
    standardize=TRUE, d=2
  )
  preds <- predict(model, Xtest)
  # Coefficients without the intercept
  B <- models(model)[[2]][, -1]
  return(list(
    err = mean(preds$classes[,2] != ytest),
    parameters = sum(B != 0),
    features = sum(colSums(B^2) != 0)
  ))
}
```

```{r}
msgl.result <- sparse.group.slope::compare(
  X, y, grid.alpha = seq(0.1, 0.001, -0.01), 
  grid.lambda = seq(0.02, 0.001, -0.001), folds=10, n.boostrap = 10,
  model.cv = msgl.fit.cv, model.fit_evaluate = msgl.fit_evaluate
)
```

# Sparse Group SLOPE
```{r}
fit.cv <- function(X, y, grid.alpha, grid.lambda, cv.indices) {
    library(purrr)
    library(sparse.group.slope)
    cvs <- matrix(-1, length(grid.alpha), length(grid.lambda))
    n <- nrow(X)
    d <- ncol(X)
    L <- nlevels(y)

    Binits <- as.vector(rep(0, length(cv.indices)), "list")
    for (i in seq_along(grid.alpha)) {
        alpha <- grid.alpha[i]
        for (j in seq_along(grid.lambda)) {
            A <- grid.lambda[j]
            lambda <- alpha * A * sapply(
                1:d, function(i) (sqrt(log(d * exp(1) / i) / n))
            )
            kappa <- (1 - alpha) * A * sapply(
                1:L, function(j) (sqrt(log(L * exp(1) / j) / n ))
            )
            fold <- function(i){
                test.group <- cv.indices[[i]]
                model <- sparse.group.slope::SparseGroupSLOPE(
                    X = X[-test.group, ], y = diag(L)[y[-test.group], ],
                    eta = 0.5, tol = 1e-4, monotone = F, accelerated = T,
                    lambda = lambda, alpha = kappa, BInit = Binits[[i]],
                    maxIter = 100000, standardize = FALSE, minStepSize = 1e-5,
                    stepSize = 0.001,
                )

                list(
                    B = model$coefficient,
                    A = A,
                    non_zeros = sum(model$coefficients != 0),
                    non_zero_groups = sum(apply(model$coefficients != 0, 1, any)),
                    iters = model$iterations,
                    error = mean(apply(predict.sgs(model, X[test.group,]), 1, which.max) != as.numeric(y[test.group]))
                )
            }

            res <- lapply(1:length(cv.indices), fold)
            cvs[i, j] <- mean(unlist(map(res, pluck, "error")))
        }
    }
    return(cvs)
}
```

```{r}
sgs.fit_evaluate <- function(Xtrain, ytrain, Xtest, ytest, alpha, lambda) {
  library(sparse.group.slope)
  A <- lambda
  lambda <- alpha * A * sapply(
      1:d, function(i) (sqrt(log(d * exp(1) / i) / n))
  )
  kappa <- (1 - alpha) * A * sapply(
      1:L, function(j) (sqrt(log(L * exp(1) / j) / n ))
  )
  model <- sparse.group.slope::SparseGroupSLOPE(
      X = Xtrain, y = diag(L)[ytrain, ],
      eta = 0.5, tol = 1e-4, monotone = F, accelerated = T,
      lambda = lambda, alpha = kappa, BInit = Binits[[i]],
      maxIter = 100000, standardize = FALSE, minStepSize = 1e-5,
      stepSize = 0.001,
  )

  return(list(
    err = mean(
      apply(predict.sgs(model, Xtest), 1, which.max) != as.numeric(ytest)
    ),
    parameters = sum(model$coefficients != 0),
    features = sum(colSums(model$coefficients^2) != 0)
  ))
}
```

```{r}
sgs.cvs <- fit.cv(
  X, y, grid.alpha = seq(0.91, 0.01, -0.05),
  grid.lambda = seq(0.15, 0.01, -0.01),
  cv.indices = msgl.result$indices
)
```

```{r}
```

```{r}
library(purrr)
library(sparse.group.slope)
n <- nrow(X)
d <- ncol(X)
L <- nlevels(y)

idx <- sample(1:n, n)
folds <- 10
fold.size <- ceiling(n/folds)
As <- seq(0.15, 0.01, -0.01)

for (w in seq(0.01, 0.91, 0.1)) {
  cvs <- rep(0, length(As))
  all_results <- list()
  Binits <- as.list(rep(0, folds))


  for (j in 1:length(As)) {
    A <- As[j]

    lambda <- (1-w)*A*sapply(1:d, function(i)(sqrt(log(d*exp(1)/i)/n)))
    alpha <- w*A*sapply(1:L, function(j)(sqrt(log(L*exp(1)/j)/n)))

    fold <- function(i){
      test.group <- fit.cv$cv.indices[[i]]
      model <- sparse.group.slope::SparseGroupSLOPE(
        X=X[-test.group,], y=diag(L)[y[-test.group],], eta=0.5, tol = 1e-4,
        monotone=F, accelerated=T, lambda = lambda, alpha = alpha, BInit = Binits[[i]], #B
        maxIter=50000, standardize=FALSE, minStepSize = 1e-5, stepSize = 0.001,
        )
      list(
        B = model$coefficient,
        A = A,
        non_zeros = sum(model$coefficients != 0),
        non_zero_groups = sum(apply(model$coefficients != 0, 1, any)),
        iters = model$iterations,
        error = mean(apply(predict.sgs(model, X[test.group,]), 1, which.max) != as.numeric(y[test.group]))
      )
    }

    res <- lapply(1:folds, fold)
    all_results <- append(all_results, res)
    Binits <- map(res, pluck, "B")
    iters <- unlist(map(res, pluck, "iters"))
    cvs[j] <- mean(unlist(map(res, pluck, "error")))
  }
  print(paste(w, min(cvs)))
}
```
