---
title: "Cancer Sites"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{simulation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(GEOquery)
library(dplyr)
library(stringr)
options(error = quote(dump.frames()))
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r, include=FALSE}
Sys.setenv(
  MKL_THREADING_LAYER="GNU",
  OPENBLAS_NUM_THREADS=5
)
```

# Process Data

```{r}
gse <- getGEO("GSE2564",GSEMatrix=TRUE)
```

```{r}
usedFeatures <- featureData(gse[[1]])$USAGE == "Used"
usedSampled <- phenoData(gse[[1]])$organism_ch1 == "Homo sapiens"
limited <- gse[[1]][usedFeatures,usedSampled]

classes <- pData(phenoData(limited)) %>% 
  mutate(class=str_extract(title, "[^_]+_[^_]+")) %>% 
  group_by(class) %>% 
  mutate(count=n()) %>% 
  ungroup() %>% 
  mutate(tumor_or_normal=str_starts(class, "T_") | str_starts(class, "N_")) %>%
  select(geo_accession, class, count, tumor_or_normal)
  
selected <- gse[[1]][, classes$geo_accession[(classes$count >= 5) & classes$tumor_or_normal]]

X <- t(exprs(selected))
X <- X[,!apply((X == 0), 2, all)]
y <- as.factor((pData(phenoData(selected)) %>% 
  mutate(class=str_extract(title, "[^_]+_[^_]+")) %>% 
  select(class))$class)
```

# Models

```{r}
library(caret)

trControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10,
                           verboseIter=TRUE)
```
## Linear

### Sparse Group LASSO

```{r}
sgl <- list(
 type="Classification",
 library="msgl",
 loop=NULL,
 parameters = data.frame(
   parameter=c("lambda", "alpha"),
   class = rep("numeric", 2),
   label = c("Regularization", "Balance")
 ),
 grid = function(x, y, len=NULL, search="grid") {
    stop("must provide tuneGrid")
 },
 fit = function(x, y, wts, param, lev, last, weights, classProbs, ...) {
   library(msgl)
   model <- msgl::fit(
    x, y, alpha = param$alpha, lambda = c(param$lambda, param$lambda), 
    standardize=TRUE, d=2
  )
 },
 predict = function(modelFit, newdata, preProc, submodels) {
   library(msgl)
   predict(modelFit, newdata)$classes[,2]
  },
  prob = function(modelFit, newdata, preProc, submodels) {
   stop("not implemented")
 },
 levels = function(x) {levels(x)}
)
```

```{r}
library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

sglmod <- train(X, y, 
                method = sgl, 
                metric = "Accuracy",
                tuneGrid = expand.grid(
                  lambda= seq(0.02, 0.001, -0.001),
                  alpha= seq(0.1, 0.001, -0.01),
                ),
                trControl = trControl)

stopCluster(cl)
```

### Group LASSO

```{r}
gl <- list(
 type="Classification",
 library="msgl",
 loop=NULL,
 parameters = data.frame(
   parameter=c("lambda"),
   class = rep("numeric", 1),
   label = c("Regularization")
 ),
 grid = function(x, y, len=NULL, search="grid") {
    stop("must provide tuneGrid")
 },
 fit = function(x, y, wts, param, lev, last, weights, classProbs, ...) {
   library(msgl)
   model <- msgl::fit(
    x, y, alpha = 1, lambda = c(param$lambda, param$lambda), 
    standardize=TRUE, d=2
  )
 },
 predict = function(modelFit, newdata, preProc, submodels) {
   library(msgl)
   predict(modelFit, newdata)$classes[,2]
  },
  prob = function(modelFit, newdata, preProc, submodels) {
   stop("not implemented")
 },
 levels = function(x) {levels(x)}
)
```

```{r}
library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

glmod <- train(X, y, 
                method = gl, 
                metric = "Accuracy",
                tuneGrid = expand.grid(
                  lambda= seq(0.91, 0.01, -0.3),
                ),
                trControl = trControl)

stopCluster(cl)
```

### Sparse Group SLOPE


```{r}
sgs <- list(
 type="Classification",
 library="sparse.group.slope",
 loop=NULL,
 parameters = data.frame(
   parameter=c("lambda", "alpha", "maxIter", "stepSize"),
   class = rep("numeric", 4),
   label = c("Regularization", "Balance", "Iterations", "stepSize")
 ),
 grid = function(x, y, len=NULL, search="grid") {
    stop("must provide tuneGrid")
 },
 fit = function(x, y, wts, param, lev, last, weights, classProbs, ...) {
   n <- nrow(x)
   d <- ncol(x)
   L <- nlevels(y)
   lambda <- param$alpha * param$lambda * sapply(
      1:d, function(i) (sqrt(log(d * exp(1) / i) / n))
    )
    kappa <- (1 - param$alpha) * param$lambda * sapply(
      1:L, function(j) (sqrt(log(L * exp(1) / j) / n ))
    )
    sparse.group.slope::SparseGroupSLOPE(
      as.matrix(x), y = diag(L)[y,],
      eta = 0.5, tol = 1e-4, monotone = F, accelerated = T,
      lambda = lambda, alpha = kappa,
      maxIter = param$maxIter, standardize = FALSE, 
      minStepSize = 1e-5, stepSize = param$stepSize, BInit=0, ...
    )
 },
 predict = function(modelFit, newdata, preProc, submodels) {
   library(sparse.group.slope)
   apply(predict.sgs(modelFit, as.matrix(newdata)), 1, which.max)
  },
  prob = function(modelFit, newdata, preProc, submodels) {
   stop("need to check implementation")
   p <- exp(predict.sgs(modelFit, as.matrix(newdata)))
   sweep(p, 1, colSums(p), '/')
 },
 levels = function(x) {levels(x)}
)
```


```{r}
library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

sgsmod <- train(X, y, 
                method = sgs, 
                metric = "Accuracy",
                tuneGrid = expand.grid(
                  lambda= seq(0.91, 0.01, -0.3),
                  alpha= seq(0.15, 0.01, -0.05),
                  maxIter = c(10000),
                  stepSize = c(0.001)
                ),
                trControl = trControl)

stopCluster(cl)
```
## Trees

### XGboost

```{r}
tuneGrid <- expand.grid(nrounds = c(50, 100, 200, 1000),
                        max_depth = c(3, 5, 10),
                        eta = c(0.01, 0.05, 0.1),
                        gamma = c(0.01, 0.1),
                        colsample_bytree = c(0.75, 1.0),
                        min_child_weight = 0,
                        subsample = c(0.5, 1.0)
          )

xgbmod <- train(X, y,
                 method = "xgbTree", 
                 trControl = trControl,
                 metric = "Accuracy",
                 #tuneGrid = tuneGrid,
                 verbosity = 0
                 )
```

### Random Forest

```{r}
trmod <- train(X, y,
                 method = "tr", 
                 trControl = trControl,
                 metric = "Accuracy",
                 #tuneGrid = tuneGrid,
                 verbosity = 0
                 )
```

## Summary

```{r}
res <- resamples(list(
  sparseGroupSLOPE = sgsmod, 
  groupSLOPE = gsmod, 
  sparseGroupLasso = sglmod, 
  groupLasso = glmod, 
  xgboost = xgbmod, 
  randomForest = trmod
  ))
summary(res)
```

