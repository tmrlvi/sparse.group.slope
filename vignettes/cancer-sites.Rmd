---
title: "Cancer Sites"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{simulation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(GEOquery)
library(dplyr)
library(stringr)
options(error = quote(dump.frames()))
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


# Process Data

```{r}
gse <- getGEO("GSE2564",GSEMatrix=TRUE)
```

```{r}
usedFeatures <- featureData(gse[[1]])$USAGE == "Used"
usedSampled <- phenoData(gse[[1]])$organism_ch1 == "Homo sapiens"
limited <- gse[[1]][usedFeatures,usedSampled]

classes <- pData(phenoData(limited)) %>% 
  mutate(class=str_extract(title, "[^_]+_[^_]+")) %>% 
  group_by(class) %>% 
  mutate(count=n()) %>% 
  ungroup() %>% 
  mutate(tumor_or_normal=str_starts(class, "T_") | str_starts(class, "N_")) %>%
  select(geo_accession, class, count, tumor_or_normal)
  
selected <- gse[[1]][, classes$geo_accession[(classes$count >= 5) & classes$tumor_or_normal]]

X <- t(exprs(selected))
X <- X[,!apply((X == 0), 2, all)]
y <- as.factor((pData(phenoData(selected)) %>% 
  mutate(class=str_extract(title, "[^_]+_[^_]+")) %>% 
  select(class))$class)
```

# Models

```{r}
library(caret)

cvRounds <- 10
# Estimating error in the final model
repeats <- 10

trControl <- trainControl(method = "cv", number = cvRounds)

repeats <- createDataPartition(y, p = .75, list = FALSE, times=repeats)
result <- data.frame(row.names=1:repeats)
```
## Linear

### Sparse Group LASSO

```{r}
sgl <- list(
 type="Classification",
 library="msgl",
 loop=NULL,
 parameters = data.frame(
   parameter=c("lambda", "alpha"),
   class = rep("numeric", 2),
   label = c("Regularization", "Balance")
 ),
 grid = function(x, y, len=NULL, search="grid") {
    stop("must provide tuneGrid")
 },
 fit = function(x, y, wts, param, lev, last, weights, classProbs, ...) {
   library(msgl)
   model <- msgl::fit(
    x, y, alpha = param$alpha, lambda = c(param$lambda, param$lambda), 
    standardize=TRUE, d=2
  )
 },
 predict = function(modelFit, newdata, preProc, submodels) {
   library(msgl)
   predict(modelFit, newdata)$classes[,2]
  },
  prob = function(modelFit, newdata, preProc, submodels) {
   stop("not implemented")
 },
 levels = function(x) {levels(x)}
)
```

```{r}
library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

result["sparseGroupLasso"] <- NA
for (i in 1:ncol(repeats)) {
  idx <- repeats[,i]
  sglmod <- train(X[idx,], y[idx], 
                  method = sgl, 
                  metric = "Accuracy",
                  tuneGrid = expand.grid(
                    lambda= seq(0.02, 0.001, -0.001),
                    alpha= seq(0.1, 0.001, -0.01),
                  ),
                  trControl = trControl)
  pred <- predict(sglmod, X[-idx,])
  result[i,"sparseGroupLasso"] <- mean(pred != y[-idx])
}

stopCluster(cl)
```

### Group LASSO

```{r}
gl <- list(
 type="Classification",
 library="msgl",
 loop=NULL,
 parameters = data.frame(
   parameter=c("lambda"),
   class = rep("numeric", 1),
   label = c("Regularization")
 ),
 grid = function(x, y, len=NULL, search="grid") {
    stop("must provide tuneGrid")
 },
 fit = function(x, y, wts, param, lev, last, weights, classProbs, ...) {
   library(msgl)
   model <- msgl::fit(
    x, y, alpha = 1, lambda = c(param$lambda, param$lambda), 
    standardize=TRUE, d=2
  )
 },
 predict = function(modelFit, newdata, preProc, submodels) {
   library(msgl)
   predict(modelFit, newdata)$classes[,2]
  },
  prob = function(modelFit, newdata, preProc, submodels) {
   stop("not implemented")
 },
 levels = function(x) {levels(x)}
)
```

```{r}
library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)

result["groupLasso"] <- NA
for (i in 1:ncol(repeats)) {
  idx <- repeats[,i]
  glmod <- train(X[idx,], y[idx],
                  method = gl, 
                  metric = "Accuracy",
                  tuneGrid = expand.grid(
                    lambda= seq(0.91, 0.01, -0.3),
                  ),
                  trControl = trControl)
  pred <- predict(glmod, X[-idx,])
  result[i,"groupLasso"] <- mean(pred != y[-idx])
}

stopCluster(cl)
```

### Sparse Group SLOPE


```{r}
sgs <- list(
 type="Classification",
 library="sparse.group.slope",
 loop=NULL,
 parameters = data.frame(
   parameter=c("lambda", "alpha", "maxIter", "stepSize"),
   class = rep("numeric", 4),
   label = c("Regularization", "Balance", "Iterations", "stepSize")
 ),
 grid = function(x, y, len=NULL, search="grid") {
    stop("must provide tuneGrid")
 },
 fit = function(x, y, wts, param, lev, last, weights, classProbs, ...) {
   n <- nrow(x)
   d <- ncol(x)
   L <- nlevels(y)
   lambda <- param$alpha * param$lambda * sapply(
      1:d, function(i) (sqrt(log(d * exp(1) / i) / n))
    )
    kappa <- (1 - param$alpha) * param$lambda * sapply(
      1:L, function(j) (sqrt(log(L * exp(1) / j) / n ))
    )
    list(
      model = sparse.group.slope::SparseGroupSLOPE(
                as.matrix(x), y = diag(L)[y,],
                eta = 0.5, tol = 1e-4, monotone = F, accelerated = T,
                lambda = lambda, alpha = kappa,
                maxIter = param$maxIter, standardize = FALSE, 
                minStepSize = 1e-5, stepSize = param$stepSize, BInit=0, ...
              ),
      levels=levels(y)
    )
 },
 predict = function(modelFit, newdata, preProc, submodels) {
   library(sparse.group.slope)
   factor(modelFit$levels[apply(predict.sgs(modelFit$model, as.matrix(newdata)), 1, which.max)])
  },
  prob = function(modelFit, newdata, preProc, submodels) {
   stop("need to check implementation")
   p <- exp(predict.sgs(modelFit, as.matrix(newdata)))
   sweep(p, 1, colSums(p), '/')
 },
 levels = function(x) {levels(x)}
)
```


```{r}
library(doParallel)
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

result["sgs"] <- NA
for (i in 1:ncol(repeats)) {
  idx <- repeats[,i]
  sgsmod <- train(X[idx,], y[idx], 
                  method = sgs, 
                  metric = "Accuracy",
                  tuneGrid = expand.grid(
                    lambda= seq(0.91, 0.01, -0.3),
                    alpha= seq(0.15, 0.01, -0.05),
                    maxIter = c(100),
                    stepSize = c(0.001)
                  ),
                  trControl = trControl)
  pred <- predict(sgsmod, X[-idx,])
  result[i,"sgs"] <- mean(pred != y[-idx])
}

stopCluster(cl)
```
## Trees

### XGboost

```{r}
result["xgboost"] <- NA
for (i in 1:ncol(repeats)) {
  idx <- repeats[,i]
  xgbmod <- train(X[idx,], y[idx],
                   method = "xgbTree", 
                   trControl = trControl,
                   metric = "Accuracy",
                   #tuneGrid = tuneGrid,
                   verbosity = 0
                   )
  pred <- predict(xgbmod, X[-idx,])
  result[i,"xgboost"] <- mean(pred != y[-idx])
}
```

### Random Forest

```{r}
result["randomForest"] <- NA
for (i in 1:ncol(repeats)) {
  idx <- repeats[,i]
  rfmod <- train(X[idx,], y[idx],
                   method = "rf", 
                   trControl = trControl,
                   metric = "Accuracy",
                   #tuneGrid = tuneGrid,
                   verbosity = 0
                   )
  pred <- predict(trmod, X[-idx,])
  result[i,"randomForest"] <- mean(pred != y[-idx,])
}
```

## Summary

```{r}
print("Mean", colMeans(result))
print("Std:", mean(sweep(result, 2, colMeans(result), "-")^2))
```

