---
title: "simulation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{simulation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(sparse.group.slope)
```

```{r generate data}
n <- 1000
d <- 100
L <- 10
d0 <- 10          # row-sparsity
L0 <- rep(2, d0)  # per-row-sparsity

B <- 10*generate.doubly.sparse(L, d, d0, L0)
X <- matrix(rnorm(n*d), nrow=n, ncol=d)
#X <- diag(100)

p <- exp(X%*%B)/rowSums(exp(X%*%B))
#Y <- t(apply(p, MARGIN = 1, function(row)(rmultinom(1, 1, row))))
Y <- apply(p, MARGIN = 1, function(row)(which.max(rmultinom(1, 1, row))))
Y.test <- apply(p, MARGIN = 1, function(row)(which.max(rmultinom(1, 1, row))))

Y.gaussian <- X%*%B + matrix(rnorm(n*L), nrow=n, ncol=L) 
```

```{r}
mean(apply(X%*%B, MARGIN=1, FUN=which.max) == Y.test)
```
```{r}
A<- 1/5
b <- c(1, rep(0, L-1))
#alpha <- A*sapply(1:L, function(j)(sqrt(log(L*exp(1)/j)/n)))
alpha <- sapply(1:L, function(j)(2^(-j)))
norm <- slope.norm(alpha)

for (a in seq(-0.1,0.1,0.00001)){
  C <- a/(sum(abs(b) > 0) > 0)
  one <- rep(1, L)
  #family$primal(X%*%(B + C%*%t(one)), diag(max(Y))[Y,])
  if (norm$evaluate(abs(b) - C) <= norm$evaluate(abs(b))){
    print(a)
  }
}
```

```{r}
A<- 1/5
lambda <- A*sapply(1:d, function(i)(sqrt(log(d*exp(1)/i)/n)))
alpha <- sapply(1:L, function(j)(2^(-j)))
norm <- sgs.norm(lambda = lambda, alpha = alpha)

for (a in seq(-0.1,0.1,0.0001)){
  C <- a*(rowSums(abs(B) > 0) > 0)
  one <- rep(1, L)
  #family$primal(X%*%(B + C%*%t(one)), diag(max(Y))[Y,])
  if (norm$evaluate(abs(B) + C%*%t(one)) <= norm$evaluate(abs(B))){
    print(a)
  }
}
```

```{r}
A<- 1/5
lambda <- A*sapply(1:d, function(i)(sqrt(log(d*exp(1)/i)/n)))
alpha <- A*sapply(1:L, function(j)(sqrt(log(L*exp(1)/j)/n)))
norm <- sgs.norm(lambda = lambda, alpha = alpha)

model <- sparse.group.slope::mfista(X=X, y=diag(max(Y))[Y,], family =multinomial(), norm = norm, step.size.init=20, eta=0.99, tol = 1e-8, max.iter=200, monotone=F, accelerated=F, B.init=B)

family$primal(X%*%B, diag(max(Y))[Y,]) < family$primal(X%*%model$B, diag(max(Y))[Y,]) 
norm$evaluate(B) < norm$evaluate(model$B)
family$primal(X%*%B, diag(max(Y))[Y,]) + nrow(X)*norm$evaluate(B) - (family$primal(X%*%model$B, diag(max(Y))[Y,]) + nrow(X)*norm$evaluate(model$B))
unique(rowSums(abs(model$B) > 0))
sum(rowSums(abs(model$B)) > 0)
```



```{r train model}
lambda <- sapply(1:d, function(i)(sqrt(log(d*exp(1)/i)/n)))
alpha <- sapply(1:L, function(j)(sqrt(log(L*exp(1)/j)/n)))
gammas <- exp(-1*(-3:10))
etas <- (1:10)/10
res <- cv.sgs(X=X, y=diag(max(Y))[Y,], lambda=lambda, alpha=alpha, gammas=gammas, etas=etas, K=3)
#model <- sparse.group.slope::mfista(X=X, y=diag(max(Y))[Y,], family =multinomial(), norm = sgs.norm(lambda = lambda, alpha = alpha), step.size.init=20, eta=0.99, tol = 1e-8, max.iter=200, monotone=F, accelerated=F)
```

```{r}
print(sum(abs(model1$B - model3$B)))
print(sum(abs(model1$B - model4$B)))
print(sum(abs(model1$B - model5$B)))
print(sum(abs(model3$B - model4$B)))
print(sum(abs(model3$B - model5$B)))
print(sum(abs(model4$B - model5$B)))
```

```{r}
mean(apply(predict(model), MARGIN=1, FUN=which.max) == Y)
```

```{r graph results}
```
